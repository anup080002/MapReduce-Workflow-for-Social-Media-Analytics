# MapReduce Workflow for Social Media Analytics

## ğŸ“ Project Overview
This project implements a multi-stage MapReduce workflow to analyze large-scale social media data. The system performs data cleansing, user activity aggregation, trending content detection, and joins with user profiles to produce enriched analytics.

## ğŸ—‚ï¸ Directory Structure
```
social_media_analytics_mapreduce/
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ social_media_logs.txt
â”‚   â””â”€â”€ user_profiles.txt
â”œâ”€â”€ mappers/
â”‚   â”œâ”€â”€ cleanse_mapper.py
â”‚   â”œâ”€â”€ aggregate_mapper.py
â”‚   â”œâ”€â”€ trending_mapper.py
â”‚   â”œâ”€â”€ join_mapper_profiles.py
â”‚   â””â”€â”€ join_mapper_activity.py
â”œâ”€â”€ reducers/
â”‚   â”œâ”€â”€ cleanse_reducer.py
â”‚   â”œâ”€â”€ aggregate_reducer.py
â”‚   â”œâ”€â”€ trending_reducer.py
â”‚   â””â”€â”€ join_reducer.py
â”œâ”€â”€ output/ (HDFS will generate this)
â”œâ”€â”€ README.md
â””â”€â”€ report.pdf / report.docx
```

## ğŸ“¥ Input Files
- `social_media_logs.txt`: Tab-separated records of user interactions.
- `user_profiles.txt`: Comma-separated user profiles with UserID, Name, and Location.

## ğŸ› ï¸ How to Run (Hadoop Streaming)
Upload input files to HDFS:
```bash
hdfs dfs -mkdir /data
hdfs dfs -put input/social_media_logs.txt /data/
hdfs dfs -put input/user_profiles.txt /data/
```

### ğŸ§¹ Job 1: Data Cleansing
```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
 -input /data/social_media_logs.txt \
 -output /output/cleansed_logs \
 -mapper mappers/cleanse_mapper.py \
 -reducer reducers/cleanse_reducer.py \
 -file mappers/cleanse_mapper.py \
 -file reducers/cleanse_re
